{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8243351,
          "sourceType": "datasetVersion",
          "datasetId": 4890239
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "\n",
        "class ChatData(Dataset):\n",
        "    def __init__(self, path:str, tokenizer):\n",
        "        self.data = json.load(open(path, \"r\"))\n",
        "\n",
        "        self.X = []\n",
        "        for i in self.data:\n",
        "            for j in i['dialog']:\n",
        "                self.X.append(j['text'])\n",
        "\n",
        "        for idx, i in enumerate(self.X):\n",
        "            try:\n",
        "                self.X[idx] = \"<startofstring> \"+i+\" <bot>: \"+self.X[idx+1]+\" <endofstring>\"\n",
        "            except:\n",
        "                break\n",
        "\n",
        "        self.X = self.X[:5000]\n",
        "\n",
        "        print(self.X[0])\n",
        "\n",
        "        self.X_encoded = tokenizer(self.X,max_length=40, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        self.input_ids = self.X_encoded['input_ids']\n",
        "        self.attention_mask = self.X_encoded['attention_mask']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.input_ids[idx], self.attention_mask[idx])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T17:03:29.905149Z",
          "iopub.execute_input": "2024-04-27T17:03:29.905533Z",
          "iopub.status.idle": "2024-04-27T17:03:29.915552Z",
          "shell.execute_reply.started": "2024-04-27T17:03:29.905502Z",
          "shell.execute_reply": "2024-04-27T17:03:29.914499Z"
        },
        "trusted": true,
        "id": "zKM7hOjkmvRZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm\n",
        "import torch\n",
        "\n",
        "def train(chatData, model, optim):\n",
        "    epochs = 12\n",
        "    for i in tqdm.tqdm(range(epochs)):\n",
        "        for X, a in chatData:\n",
        "            X = X.to(device)\n",
        "            a = a.to(device)\n",
        "            optim.zero_grad()\n",
        "            loss = model(X, attention_mask=a, labels=X).loss\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "        torch.save(model.state_dict(), \"model_state.pt\")\n",
        "        print(infer(\"hello how are you\"))\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
        "                                \"bos_token\": \"<startofstring>\",\n",
        "                                \"eos_token\": \"<endofstring>\"})\n",
        "tokenizer.add_tokens([\"<bot>:\"])\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# print(tokenizer.decode(model.generate(**tokenizer(\"hey i was good at basketball but \",\n",
        "#                          return_tensors=\"pt\"))[0]))\n",
        "\n",
        "chatData = ChatData(\"./chat_data.json\", tokenizer)\n",
        "chatData =  DataLoader(chatData, batch_size=64)\n",
        "\n",
        "model.train()\n",
        "\n",
        "optim = Adam(model.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T17:03:37.054722Z",
          "iopub.execute_input": "2024-04-27T17:03:37.055596Z",
          "iopub.status.idle": "2024-04-27T17:03:39.779554Z",
          "shell.execute_reply.started": "2024-04-27T17:03:37.055559Z",
          "shell.execute_reply": "2024-04-27T17:03:39.778709Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djwiqHgimvRb",
        "outputId": "09074325-750e-4255-9a96-38c5cc032dfb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> I love iphone! i just bought new iphone! <bot>: Thats good for you, i'm not very into new tech <endofstring>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"training .... \")\n",
        "train(chatData, model, optim)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-27T17:04:25.042480Z",
          "iopub.execute_input": "2024-04-27T17:04:25.043172Z",
          "iopub.status.idle": "2024-04-27T17:04:25.052253Z",
          "shell.execute_reply.started": "2024-04-27T17:04:25.043127Z",
          "shell.execute_reply": "2024-04-27T17:04:25.051212Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki5NtINhmvRb",
        "outputId": "ae167034-ba8f-4b9d-83ad-92a53a79ba14"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training .... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/12 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  8%|▊         | 1/12 [00:53<09:50, 53.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you  <bot>:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 17%|█▋        | 2/12 [01:36<07:53, 47.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you  <bot>:   i am a huge i am a huge, i am a huge i am a huge i am a huge huge gamer, i am a huge, i\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 25%|██▌       | 3/12 [02:19<06:48, 45.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you  <bot>:   i am not a huge i am not sure i am not a huge i am a huge i am a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 33%|███▎      | 4/12 [03:07<06:12, 46.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you  <bot>:   I do you are doing it is a little?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 42%|████▏     | 5/12 [03:58<05:35, 47.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you  <bot>:   i am not a big fan of the nighing, i am not a huge fan of a huge fan of a huge fan of a fan of a fan of all the truth  i am a huge fan of the\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 50%|█████     | 6/12 [04:51<04:58, 49.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you  <bot>:   i am a huge gamer  i am a gamer  i am a gamer  i am a huge gamer  ok\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 58%|█████▊    | 7/12 [05:40<04:07, 49.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you  <bot>:   i am not sure what you mean  i am not sure you are right man, i am not sure you are you a little man in a man\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 67%|██████▋   | 8/12 [06:23<03:09, 47.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you  <bot>:   I am a huge fan of it's not fan of it's fan of all the news. i am a fan of all about that fan, fan. i am a fan of it. i love it. fan of fan. i love it's fan. fan fan.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 75%|███████▌  | 9/12 [07:17<02:28, 49.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you  <bot>:   Hi, how are doing?  Hi, how are doing great, how are doing you doing?  Hi, how are doing?  I just finished doing?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 83%|████████▎ | 10/12 [08:00<01:35, 47.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you  <bot>:   I am a huge fan of all animals. are you a lot of animals?  I have a lot of them. are you?  I\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 92%|█████████▏| 11/12 [08:48<00:47, 47.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you  <bot>:   I am a huge fan of all animals. I love animals. are you a 2pac, or are you?  I am a 2pac, i am a golden?  I am a golden retriever  I am a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "100%|██████████| 12/12 [09:37<00:00, 48.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you  <bot>:   I am a student, i am a student       I am a real student\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(inp):\n",
        "    inp = \"<startofstring> \" + inp + \" <bot>: \"\n",
        "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
        "    X = inp[\"input_ids\"].to(device)\n",
        "    a = inp[\"attention_mask\"].to(device)\n",
        "    output = model.generate(X, attention_mask=a, max_length=100, num_return_sequences=1)\n",
        "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return output_text.split(\"<endofstring>\")[0].strip()"
      ],
      "metadata": {
        "id": "C74uHtN4_Pwh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"infer from model : \")\n",
        "while True:\n",
        "  inp = input()\n",
        "  print(infer(inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "rqTu8MXEmvRc",
        "outputId": "21380a95-bb10-4f7a-a292-534d63433754"
      },
      "execution_count": 23,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "infer from model : \n",
            "hello\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello  <bot>:   Hi, how are doing?  Hi, how are doing?  Hi, how are doing?  Hi, how are doing?  Hi\n",
            "what are you doing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what are you doing  <bot>:   I am doing well. I am just hanging out with my dog.  I am a dog.  I am a dog.   I am a dog.\n",
            "do you love music\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "do you love music  <bot>:   I do, i do you speak a french???  I speak french?  Yes, but\n",
            "do you like coding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do you like coding  <bot>:   i do you like music?  i like to listen music  i like to music  i like to play the best  i like to play the piano?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b1c943b2c177>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"infer from model : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cJ_ymd5N3Hxp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}